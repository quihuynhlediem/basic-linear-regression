{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# url for train data: https://transfer.sh/RyD0RfrNuu/mnist_train.csv -P data/\n",
    "# url for test data: https://transfer.sh/NC2NJipGvn/mnist_test.csv -P data/\n",
    "# Using transfer.sh to host the data\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "\n",
    "filepath_train = 'data/mnist_train.csv'\n",
    "url_train = 'https://transfer.sh/RyD0RfrNuu/mnist_train.csv'\n",
    "filepath_test = 'data/mnist_test.csv'\n",
    "url_test = 'https://transfer.sh/NC2NJipGvn/mnist_test.csv'\n",
    "\n",
    "if not os.path.exists(filepath_train):\n",
    "  urllib.request.urlretrieve(url_train, filepath_train)\n",
    "\n",
    "if not os.path.exists(filepath_test):\n",
    "  urllib.request.urlretrieve(url_test, filepath_test)\n",
    "\n",
    "\n",
    "data = pd.read_csv('data/mnist_train.csv')\n",
    "# data = pd.read_csv(url)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 10 model, model i answer whether the picture is of the i-th digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "class LossFunction:\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def loss(self, a : np.ndarray): # a is the coefficient vector\n",
    "        prob = sigmoid(np.dot(self.X, a))\n",
    "        values = - self.y * np.log(prob) - (1 - self.y) * np.log(1 - prob)\n",
    "        return np.nansum(values) / self.y.shape[0]\n",
    "        # Vi han che gia tri ham loss function nen can chia trung binh\n",
    "        # de tranh bi tran so giong normalize data\n",
    "\n",
    "    def gradient(self, a : np.ndarray):\n",
    "        prob = sigmoid(np.dot(self.X, a)) # aT * X or XT * a = tich vo huong\n",
    "        sub_coefficient = -(self.y - prob) \n",
    "        return np.dot(self.X.T, sub_coefficient) / self.y.shape[0]\n",
    "    \n",
    "    def precision(self, a : np.ndarray):\n",
    "        prob = sigmoid(np.dot(self.X, a))\n",
    "        prob = np.array(prob >= 0.5, dtype=np.int32)\n",
    "        return np.sum(prob == self.y) / self.y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 785), (60000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.drop('label', axis=1).values # Data ko chua label\n",
    "# X shape m * 784\n",
    "# Axis = 1 mean drop column\n",
    "# Axis = 0 mean drop row\n",
    "\n",
    "#append a column of 1s for ax + by + c (x, y, 1) and (a, b, c)\n",
    "X = np.concatenate((X, np.ones((X.shape[0], 1))), axis=1)\n",
    "y = data['label'].values\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test code cell\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_functions = []\n",
    "for digit in range(10):\n",
    "    y_digit = np.array(y == digit, dtype=np.int32) # y_digit is a vector of 0 and 1\n",
    "    loss_functions.append(LossFunction(X, y_digit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.LossFunction at 0x10657e750>,\n",
       " <__main__.LossFunction at 0x13207fcd0>,\n",
       " <__main__.LossFunction at 0x1320b79d0>,\n",
       " <__main__.LossFunction at 0x1065bf650>,\n",
       " <__main__.LossFunction at 0x106c8ca10>,\n",
       " <__main__.LossFunction at 0x106c8fa90>,\n",
       " <__main__.LossFunction at 0x13770b9d0>,\n",
       " <__main__.LossFunction at 0x13770b7d0>,\n",
       " <__main__.LossFunction at 0x137709090>,\n",
       " <__main__.LossFunction at 0x106c8ff50>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(loss_func, starting_point, learning_rate = 0.00001, num_steps = 40, precision=0.00001):\n",
    "    cur_point = starting_point\n",
    "    for i in range(num_steps):\n",
    "        grad = loss_func.gradient(cur_point)\n",
    "        # print(\"Iteration {}: loss = {}, precision = {}\".format(i, loss_func.loss(cur_point), loss_func.precision(cur_point)))\n",
    "        cur_point = cur_point - learning_rate * grad\n",
    "        if np.linalg.norm(grad) < precision:\n",
    "            break\n",
    "    return cur_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for digit 0: 0.9843333333333333\n",
      "Accuracy for digit 1: 0.98625\n",
      "Accuracy for digit 2: 0.9673333333333334\n",
      "Accuracy for digit 3: 0.96225\n",
      "Accuracy for digit 4: 0.9729333333333333\n",
      "Accuracy for digit 5: 0.9528166666666666\n",
      "Accuracy for digit 6: 0.9795666666666667\n",
      "Accuracy for digit 7: 0.9779166666666667\n",
      "Accuracy for digit 8: 0.9358166666666666\n",
      "Accuracy for digit 9: 0.9471333333333334\n"
     ]
    }
   ],
   "source": [
    "optimal_points = []\n",
    "\n",
    "for digit in range(10):\n",
    "    optimal_point = gradient_descent(loss_functions[digit], np.zeros(X.shape[1]))\n",
    "    print(\"Accuracy for digit {}: {}\".format(digit, loss_functions[digit].precision(optimal_point)))\n",
    "    optimal_points.append(optimal_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m infer(X, optimal_points)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(y_pred \u001b[38;5;241m==\u001b[39m y) \u001b[38;5;241m/\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m evaluate(\u001b[43mX_test\u001b[49m, y_test, optimal_points)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "def evaluate(X, y, optimal_points):\n",
    "    y_pred = infer(X, optimal_points)\n",
    "    return np.sum(y_pred == y) / y.shape[0]\n",
    "evaluate(X_test, y_test, optimal_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(X, optimal_points):\n",
    "    prob = np.zeros((X.shape[0], 10))\n",
    "    for digit in range(10):\n",
    "        prob[:, digit] = sigmoid(np.dot(X, optimal_points[digit]))\n",
    "    # print(prob.shape)\n",
    "    return np.argmax(prob, axis=1) # return index of max value in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = infer(X, optimal_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 4, ..., 5, 6, 8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_digit(data, row_id):\n",
    "    row = data.iloc[row_id]\n",
    "    label = row['label']\n",
    "    image = row.drop('label').values.reshape(28, 28)\n",
    "    plt.title('Digit Label = {}'.format(label))\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('data/mnist_test.csv')\n",
    "X_test = data_test.drop('label', axis=1).values\n",
    "X_test = np.concatenate((X_test, np.ones((X_test.shape[0], 1))), axis=1)\n",
    "y_test = data_test['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_test = 5999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAicklEQVR4nO3dfXBV9Z3H8c8NkMtDkosx5OGuIQZQYHhqN2JAFFBSAnVVKjP1aR3oWlAabCFtdeMTIrapdIqULqC7s0O2FlTcUWidFRaBBLVAlyhlrRAJEwELicqU3BAgYPLbPxjuek0CnHDD9ya8XzNnhnvO73vPl5MjH889v3vic845AQBwicVZNwAAuDwRQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBA6JCefvpp+Xy+NtWWlJTI5/Ppk08+iW5TbTR+/HgNHTo0qu959dVXa/r06VF9TyDaCCCYOxsIZ5fu3bsrGAwqPz9fS5YsUV1dXbv3sGzZMpWUlFzweJ/Pp9mzZ7dfQx3MJ598EvEz/PoyY8YM6xYRg7paNwCc9cwzzyg7O1unT59WdXW1SktLNWfOHC1atEi///3vNXz48PDYJ554Qv/8z//cpv3cf//9uvvuu+X3+8Prli1bppSUFK4a2qhPnz566aWXmq1ft26dVq5cqYkTJxp0hVhHACFmTJ48Wdddd134dVFRkTZt2qR/+Id/0O23367du3erR48ekqSuXbuqa9e2nb5dunRRly5dotIzzujVq5f+8R//sdn6kpISJSUl6bbbbjPoCrGOj+AQ02655RY9+eST2r9/v373u9+F17d0D+jEiRP64Q9/qJSUFCUmJur222/XX//6V/l8Pj399NPhcV+/B3T11VfrL3/5i8rKysIfGY0fP/6ie1+7dq1uvfVWBYNB+f1+9e/fXwsWLFBjY2OL48vLy3XDDTeoR48eys7O1gsvvNBsTENDg+bNm6cBAwbI7/crMzNTjzzyiBoaGi6632g7fPiwNm/erDvvvFPdu3e3bgcxiCsgxLz7779fjz32mP77v//7nPcSpk+frtWrV+v+++/XqFGjVFZWpltvvfW877948WI9/PDDSkhI0OOPPy5JSktLu+i+S0pKlJCQoMLCQiUkJGjTpk166qmnFAqF9Mtf/jJi7N/+9jd9+9vf1ne/+13dc889Wr16tWbNmqX4+Hj90z/9kySpqalJt99+u959913NnDlTgwcP1v/+7//q+eef18cff6w1a9Z47vFvf/tbq4H4VT179lTPnj09vfcrr7yipqYm3XfffZ77wmXCAcZWrFjhJLn/+Z//aXVMIBBw3/zmN8Ov582b5756+paXlztJbs6cORF106dPd5LcvHnzmu2vqqoqvG7IkCFu3LhxF9yzJFdQUHDOMcePH2+27sEHH3Q9e/Z0J0+eDK8bN26ck+R+9atfhdc1NDS4b3zjGy41NdWdOnXKOefcSy+95OLi4tw777wT8Z4vvPCCk+Tee++98LqsrCw3bdq08/49srKynKTzLl89fhcqJyfHZWRkuMbGRs+1uDxwBYQOISEh4Zyz4datWydJ+sEPfhCx/uGHH/Y0uy2azt6vkqS6ujo1NDTopptu0osvvqg9e/ZoxIgR4e1du3bVgw8+GH4dHx+vBx98ULNmzVJ5eblGjRql1157TYMHD9agQYP0xRdfhMfecsstkqTNmzfrhhtu8NTjypUrdeLEifOO69evn6f3/fjjj1VeXq65c+cqLo5P+tEyAggdwrFjx5Samtrq9v379ysuLk7Z2dkR6wcMGNDerbXqL3/5i5544glt2rRJoVAoYlttbW3E62AwqF69ekWsu/baayWdmeI8atQo7d27V7t371afPn1a3N9nn33muccxY8Z4rrkQK1eulCQ+fsM5EUCIeZ9++qlqa2tNw8Sro0ePaty4cUpKStIzzzyj/v37q3v37nr//ff16KOPqqmpyfN7NjU1adiwYVq0aFGL2zMzMz2/5+eff35B94ASEhKUkJBwwe+7atUqDRw4UDk5OZ57wuWDAELMO/v9kvz8/FbHZGVlqampSVVVVbrmmmvC6ysrKy9oH219qkJrSktLdeTIEb3++usaO3ZseH1VVVWL4w8dOqT6+vqIq6CPP/5Y0plZepLUv39//fnPf9aECROi1u/IkSO1f//+846bN29exEzCc9m+fbsqKyv1zDPPXGR36OwIIMS0TZs2acGCBcrOzj7nxzn5+fl6/PHHtWzZMj3//PPh9b/5zW8uaD+9evXS0aNHL7bdsLPfM3LOhdedOnVKy5Yta3H8l19+qRdffFGFhYXhsS+++KL69OkTvor47ne/q//6r//Sv/3bv2nmzJkR9SdOnFBTU1Ozj/HOpz3uAa1atUqSdO+993rqBZcfAggx46233tKePXv05ZdfqqamRps2bdKGDRuUlZWl3//+9+f8LklOTo6mTp2qxYsX68iRI+Fp2GevIs53xZCTk6Ply5fr2Wef1YABA5Samhq+ud+aHTt26Nlnn222fvz48brhhht0xRVXaNq0afrhD38on8+nl156KSKQvioYDOq5557TJ598omuvvVavvvqqdu7cqX/9139Vt27dJJ2Zjr569Wo99NBD2rx5s8aMGaPGxkbt2bNHq1ev1vr16yO+yHshon0PqLGxUa+++qpGjRql/v37R/W90QlZT8MDzk6LPrvEx8e79PR0961vfcv9+te/dqFQqFnN16dhO+dcfX29KygocMnJyS4hIcFNmTLFVVRUOEnuF7/4RbP9fXUadnV1tbv11ltdYmKik3TeKdk6x5TlBQsWOOece++999yoUaNcjx49XDAYdI888ohbv369k+Q2b94cfq9x48a5IUOGuB07drjRo0e77t27u6ysLPcv//IvzfZ76tQp99xzz7khQ4Y4v9/vrrjiCpeTk+Pmz5/vamtrw+MudBp2tK1bt85JckuWLLnk+0bH43Oulf8lAzqBnTt36pvf/KZ+97vfMSMLiDFM0Een0dK9jMWLFysuLi5iIgCA2MA9IHQaCxcuVHl5uW6++WZ17dpVb731lt566y3NnDmzTVOUAbQvPoJDp7FhwwbNnz9fH330kY4dO6a+ffvq/vvv1+OPP97mJ2cDaD8EEADABPeAAAAmCCAAgImY+2C8qalJhw4dUmJiYtQfjwIAaH/OOdXV1SkYDJ7zaegxF0CHDh1ixhIAdAIHDx7UVVdd1er2mPsILjEx0boFAEAUnO/f83YLoKVLl+rqq69W9+7dlZubqz/96U8XVMfHbgDQOZzv3/N2CaBXX31VhYWFmjdvnt5//32NGDFC+fn5bfqFWQCATqo9HjB3/fXXu4KCgvDrxsZGFwwGXXFx8Xlra2trL+h31LOwsLCwxPby1QfktiTqV0CnTp1SeXm58vLywuvi4uKUl5enrVu3Nhvf0NCgUCgUsQAAOr+oB9AXX3yhxsZGpaWlRaxPS0tTdXV1s/HFxcUKBALhhRlwAHB5MJ8FV1RUpNra2vBy8OBB65YAAJdA1L8HlJKSoi5duqimpiZifU1NjdLT05uN9/v98vv90W4DABDjon4FFB8fr5ycHG3cuDG8rqmpSRs3btTo0aOjvTsAQAfVLk9CKCws1LRp03Tdddfp+uuv1+LFi1VfX6/vfe977bE7AEAH1C4BdNddd+nzzz/XU089perqan3jG9/QunXrmk1MAABcvmLu9wGFQiEFAgHrNgAAF6m2tlZJSUmtbjefBQcAuDwRQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADARNQD6Omnn5bP54tYBg0aFO3dAAA6uK7t8aZDhgzR22+//f876douuwEAdGDtkgxdu3ZVenp6e7w1AKCTaJd7QHv37lUwGFS/fv1033336cCBA62ObWhoUCgUilgAAJ1f1AMoNzdXJSUlWrdunZYvX66qqirddNNNqqura3F8cXGxAoFAeMnMzIx2SwCAGORzzrn23MHRo0eVlZWlRYsW6YEHHmi2vaGhQQ0NDeHXoVCIEAKATqC2tlZJSUmtbm/32QG9e/fWtddeq8rKyha3+/1++f3+9m4DABBj2v17QMeOHdO+ffuUkZHR3rsCAHQgUQ+gn/zkJyorK9Mnn3yiP/7xj/rOd76jLl266J577on2rgAAHVjUP4L79NNPdc899+jIkSPq06ePbrzxRm3btk19+vSJ9q4AAB1Yu09C8CoUCikQCFi3AQC4SOebhMCz4AAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAICJrtYNoHULFizwXLNmzZroN9KKwYMHe6557LHHPNd89NFHnmt8Pp/nGkkaNGiQ55rXX3/dc01bfk5TpkzxXFNRUeG5RpLWr1/vuebzzz9v075w+eIKCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAmfc85ZN/FVoVBIgUDAuo2Y0NjY6LmmLT/Otj6481Ltqy37aetDOAcOHOi55siRI55rfv7zn3uuWbRokeeatv7nvWTJEs81hYWFbdoXOq/a2lolJSW1up0rIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACZ4GGkMe+uttzzX3HjjjZ5revXq5blGiu2HkR4/ftxzjSR98cUXnmuysrI81+zevdtzTVt+TldeeaXnmrbu67rrrvNc8/7773uuQcfBw0gBADGJAAIAmPAcQFu2bNFtt92mYDAon8+nNWvWRGx3zumpp55SRkaGevTooby8PO3duzda/QIAOgnPAVRfX68RI0Zo6dKlLW5fuHChlixZohdeeEHbt29Xr169lJ+fr5MnT150swCAzqOr14LJkydr8uTJLW5zzmnx4sV64okndMcdd0iSfvvb3yotLU1r1qzR3XfffXHdAgA6jajeA6qqqlJ1dbXy8vLC6wKBgHJzc7V169YWaxoaGhQKhSIWAEDnF9UAqq6uliSlpaVFrE9LSwtv+7ri4mIFAoHwkpmZGc2WAAAxynwWXFFRkWpra8PLwYMHrVsCAFwCUQ2g9PR0SVJNTU3E+pqamvC2r/P7/UpKSopYAACdX1QDKDs7W+np6dq4cWN4XSgU0vbt2zV69Oho7goA0MF5ngV37NgxVVZWhl9XVVVp586dSk5OVt++fTVnzhw9++yzuuaaa5Sdna0nn3xSwWBQU6ZMiWbfAIAOznMA7dixQzfffHP4dWFhoSRp2rRpKikp0SOPPKL6+nrNnDlTR48e1Y033qh169ape/fu0esaANDh8TDSGJaSknJJanr27Om5pq0GDx7suWbQoEGea9r6kMt33nnHc03fvn091+zZs8dzTVt+TjfddJPnGkn6z//8T881P//5zz3XPPnkk55r0HHwMFIAQEwigAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJjgadgAmikrK/Nc05YnsQ8ZMsRzDToOnoYNAIhJBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATHS1bgBA7Nm9e7fnmu9///vt0Ak6M66AAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmOBhpACiwufzea4ZO3as55otW7Z4rkFs4goIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACR5GCiAqnHOeawYNGuS5hoeRdh5cAQEATBBAAAATngNoy5Ytuu222xQMBuXz+bRmzZqI7dOnT5fP54tYJk2aFK1+AQCdhOcAqq+v14gRI7R06dJWx0yaNEmHDx8OLy+//PJFNQkA6Hw8T0KYPHmyJk+efM4xfr9f6enpbW4KAND5tcs9oNLSUqWmpmrgwIGaNWuWjhw50urYhoYGhUKhiAUA0PlFPYAmTZqk3/72t9q4caOee+45lZWVafLkyWpsbGxxfHFxsQKBQHjJzMyMdksAgBgU9e8B3X333eE/Dxs2TMOHD1f//v1VWlqqCRMmNBtfVFSkwsLC8OtQKEQIAcBloN2nYffr108pKSmqrKxscbvf71dSUlLEAgDo/No9gD799FMdOXJEGRkZ7b0rAEAH4vkjuGPHjkVczVRVVWnnzp1KTk5WcnKy5s+fr6lTpyo9PV379u3TI488ogEDBig/Pz+qjQMAOjbPAbRjxw7dfPPN4ddn799MmzZNy5cv165du/Qf//EfOnr0qILBoCZOnKgFCxbI7/dHr2sAQIfnOYDGjx9/zocOrl+//qIaAtAx+Xw+6xbQwfAsOACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACAiaj/Sm4Al6dzPSUfaAlXQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEzwMFIAzbz77ruea2bMmOG5Zs+ePZ5r0HlwBQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEDyMF0Mzu3bs91zjnPNcUFRV5rtmyZYvnGsQmroAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCY4GGkAKLC5/N5rklJSWmHTtBRcAUEADBBAAEATHgKoOLiYo0cOVKJiYlKTU3VlClTVFFRETHm5MmTKigo0JVXXqmEhARNnTpVNTU1UW0aANDxeQqgsrIyFRQUaNu2bdqwYYNOnz6tiRMnqr6+Pjxm7ty5+sMf/qDXXntNZWVlOnTokO68886oNw4A6Ng8TUJYt25dxOuSkhKlpqaqvLxcY8eOVW1trf793/9dq1at0i233CJJWrFihQYPHqxt27Zp1KhR0escANChXdQ9oNraWklScnKyJKm8vFynT59WXl5eeMygQYPUt29fbd26tcX3aGhoUCgUilgAAJ1fmwOoqalJc+bM0ZgxYzR06FBJUnV1teLj49W7d++IsWlpaaqurm7xfYqLixUIBMJLZmZmW1sCAHQgbQ6ggoICffjhh3rllVcuqoGioiLV1taGl4MHD17U+wEAOoY2fRF19uzZevPNN7VlyxZdddVV4fXp6ek6deqUjh49GnEVVFNTo/T09Bbfy+/3y+/3t6UNAEAH5ukKyDmn2bNn64033tCmTZuUnZ0dsT0nJ0fdunXTxo0bw+sqKip04MABjR49OjodAwA6BU9XQAUFBVq1apXWrl2rxMTE8H2dQCCgHj16KBAI6IEHHlBhYaGSk5OVlJSkhx9+WKNHj2YGHAAggqcAWr58uSRp/PjxEetXrFih6dOnS5Kef/55xcXFaerUqWpoaFB+fr6WLVsWlWYBAJ2HzznnrJv4qlAopEAgYN0GcFnLysryXLN9+3bPNW2ZdDRy5EjPNbBRW1urpKSkVrfzLDgAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgIk2/UZUAJ3b/v37Pde05cnWuLxxBQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMNHVugEAncM777zjuWbu3Lmea/Lz8z3XrF+/3nMN2h9XQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEzwMFIAUbFnzx7PNU1NTZ5rpkyZ4rmGh5HGJq6AAAAmCCAAgAlPAVRcXKyRI0cqMTFRqampmjJliioqKiLGjB8/Xj6fL2J56KGHoto0AKDj8xRAZWVlKigo0LZt27RhwwadPn1aEydOVH19fcS4GTNm6PDhw+Fl4cKFUW0aANDxeZqEsG7duojXJSUlSk1NVXl5ucaOHRte37NnT6Wnp0enQwBAp3RR94Bqa2slScnJyRHrV65cqZSUFA0dOlRFRUU6fvx4q+/R0NCgUCgUsQAAOr82T8NuamrSnDlzNGbMGA0dOjS8/t5771VWVpaCwaB27dqlRx99VBUVFXr99ddbfJ/i4mLNnz+/rW0AADqoNgdQQUGBPvzwQ7377rsR62fOnBn+87Bhw5SRkaEJEyZo37596t+/f7P3KSoqUmFhYfh1KBRSZmZmW9sCAHQQbQqg2bNn680339SWLVt01VVXnXNsbm6uJKmysrLFAPL7/fL7/W1pAwDQgXkKIOecHn74Yb3xxhsqLS1Vdnb2eWt27twpScrIyGhTgwCAzslTABUUFGjVqlVau3atEhMTVV1dLUkKBALq0aOH9u3bp1WrVunb3/62rrzySu3atUtz587V2LFjNXz48Hb5CwAAOiZPAbR8+XJJZ75s+lUrVqzQ9OnTFR8fr7fffluLFy9WfX29MjMzNXXqVD3xxBNRaxgA0Dl4/gjuXDIzM1VWVnZRDQEALg88DRuAmbg4719F/OpM2ws1a9YszzVofzyMFABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkeRgogKj766CPPNU1NTZ5rfvazn3muQWziCggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJmLuWXDOOesWALTBl19+6bkmFAp5rjl58qTnGtg437/nMRdAdXV11i0AaINt27Z5rrniiivaoRPEirq6OgUCgVa3+1yMXXI0NTXp0KFDSkxMlM/ni9gWCoWUmZmpgwcPKikpyahDexyHMzgOZ3AczuA4nBELx8E5p7q6OgWDQcXFtX6nJ+augOLi4nTVVVedc0xSUtJlfYKdxXE4g+NwBsfhDI7DGdbH4VxXPmcxCQEAYIIAAgCY6FAB5Pf7NW/ePPn9futWTHEczuA4nMFxOIPjcEZHOg4xNwkBAHB56FBXQACAzoMAAgCYIIAAACYIIACACQIIAGCiwwTQ0qVLdfXVV6t79+7Kzc3Vn/70J+uWLrmnn35aPp8vYhk0aJB1W+1uy5Ytuu222xQMBuXz+bRmzZqI7c45PfXUU8rIyFCPHj2Ul5envXv32jTbjs53HKZPn97s/Jg0aZJNs+2kuLhYI0eOVGJiolJTUzVlyhRVVFREjDl58qQKCgp05ZVXKiEhQVOnTlVNTY1Rx+3jQo7D+PHjm50PDz30kFHHLesQAfTqq6+qsLBQ8+bN0/vvv68RI0YoPz9fn332mXVrl9yQIUN0+PDh8PLuu+9at9Tu6uvrNWLECC1durTF7QsXLtSSJUv0wgsvaPv27erVq5fy8/M73VOTz3ccJGnSpEkR58fLL798CTtsf2VlZSooKNC2bdu0YcMGnT59WhMnTlR9fX14zNy5c/WHP/xBr732msrKynTo0CHdeeedhl1H34UcB0maMWNGxPmwcOFCo45b4TqA66+/3hUUFIRfNzY2umAw6IqLiw27uvTmzZvnRowYYd2GKUnujTfeCL9uampy6enp7pe//GV43dGjR53f73cvv/yyQYeXxtePg3POTZs2zd1xxx0m/Vj57LPPnCRXVlbmnDvzs+/WrZt77bXXwmN2797tJLmtW7datdnuvn4cnHNu3Lhx7kc/+pFdUxcg5q+ATp06pfLycuXl5YXXxcXFKS8vT1u3bjXszMbevXsVDAbVr18/3XfffTpw4IB1S6aqqqpUXV0dcX4EAgHl5uZeludHaWmpUlNTNXDgQM2aNUtHjhyxbqld1dbWSpKSk5MlSeXl5Tp9+nTE+TBo0CD17du3U58PXz8OZ61cuVIpKSkaOnSoioqKdPz4cYv2WhVzT8P+ui+++EKNjY1KS0uLWJ+WlqY9e/YYdWUjNzdXJSUlGjhwoA4fPqz58+frpptu0ocffqjExETr9kxUV1dLUovnx9ltl4tJkybpzjvvVHZ2tvbt26fHHntMkydP1tatW9WlSxfr9qKuqalJc+bM0ZgxYzR06FBJZ86H+Ph49e7dO2JsZz4fWjoOknTvvfcqKytLwWBQu3bt0qOPPqqKigq9/vrrht1GivkAwv+bPHly+M/Dhw9Xbm6usrKytHr1aj3wwAOGnSEW3H333eE/Dxs2TMOHD1f//v1VWlqqCRMmGHbWPgoKCvThhx9eFvdBz6W14zBz5szwn4cNG6aMjAxNmDBB+/btU//+/S91my2K+Y/gUlJS1KVLl2azWGpqapSenm7UVWzo3bu3rr32WlVWVlq3YubsOcD50Vy/fv2UkpLSKc+P2bNn680339TmzZsjfn9Yenq6Tp06paNHj0aM76znQ2vHoSW5ubmSFFPnQ8wHUHx8vHJycrRx48bwuqamJm3cuFGjR4827MzesWPHtG/fPmVkZFi3YiY7O1vp6ekR50coFNL27dsv+/Pj008/1ZEjRzrV+eGc0+zZs/XGG29o06ZNys7Ojtiek5Ojbt26RZwPFRUVOnDgQKc6H853HFqyc+dOSYqt88F6FsSFeOWVV5zf73clJSXuo48+cjNnznS9e/d21dXV1q1dUj/+8Y9daWmpq6qqcu+9957Ly8tzKSkp7rPPPrNurV3V1dW5Dz74wH3wwQdOklu0aJH74IMP3P79+51zzv3iF79wvXv3dmvXrnW7du1yd9xxh8vOznYnTpww7jy6znUc6urq3E9+8hO3detWV1VV5d5++23393//9+6aa65xJ0+etG49ambNmuUCgYArLS11hw8fDi/Hjx8Pj3nooYdc37593aZNm9yOHTvc6NGj3ejRow27jr7zHYfKykr3zDPPuB07driqqiq3du1a169fPzd27FjjziN1iAByzrnf/OY3rm/fvi4+Pt5df/31btu2bdYtXXJ33XWXy8jIcPHx8e7v/u7v3F133eUqKyut22p3mzdvdpKaLdOmTXPOnZmK/eSTT7q0tDTn9/vdhAkTXEVFhW3T7eBcx+H48eNu4sSJrk+fPq5bt24uKyvLzZgxo9P9T1pLf39JbsWKFeExJ06ccD/4wQ/cFVdc4Xr27Om+853vuMOHD9s13Q7OdxwOHDjgxo4d65KTk53f73cDBgxwP/3pT11tba1t41/D7wMCAJiI+XtAAIDOiQACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAm/g+D+q7Jnn8GTAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(infer(np.array([X_test[idx_test]]), optimal_points))\n",
    "draw_digit(data_test, idx_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write softmax function\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.15725929e-03, 3.98794048e-04, 3.02123377e-03, 6.96229101e-03,\n",
       "       4.54614533e-03, 1.83255720e-02, 2.82382550e-03, 9.33751398e-01,\n",
       "       5.05966692e-03, 2.19538145e-02])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute softmax function for X_test\n",
    "prob_softmax = np.array(softmax(np.dot(X_test[idx_test], np.array(\n",
    "    optimal_points).T)), dtype=float)  # T is transpose\n",
    "prob_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate sum of prob_softmax - Check if it is equal to 1\n",
    "sum_prob_softmax = np.sum(prob_softmax)\n",
    "sum_prob_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785) (60000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25.096274643901403"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    # Implement the stable version of softmax\n",
    "    return np.exp(z - np.max(z, axis=1, keepdims=True)) / np.sum(np.exp(z - np.max(z, axis=1, keepdims=True)), axis=1, keepdims=True)\n",
    "\n",
    "class LossFunction:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = np.zeros((y.shape[0], 10))\n",
    "        self.y[np.arange(y.shape[0]), y] = 1\n",
    "        print(self.X.shape, self.y.shape)\n",
    "\n",
    "    def loss(self, W : np.ndarray):\n",
    "        # Calculate the loss function\n",
    "        # W: shape 785 x 10\n",
    "        prob = softmax(np.dot(self.X, W))\n",
    "        #avoid inf\n",
    "        prob = np.clip(prob, 1e-15, 1 - 1e-15)\n",
    "        values = - np.nansum(self.y * np.log(prob))\n",
    "        return values / self.X.shape[0]\n",
    "    \n",
    "    def gradient(self, W : np.ndarray):\n",
    "        # Calculate the gradient\n",
    "        prob = softmax(np.dot(self.X, W))\n",
    "        difference = -(self.y - prob) \n",
    "        return np.dot(self.X.T, difference) / self.X.shape[0]\n",
    "    \n",
    "    def precision(self, W : np.ndarray):\n",
    "        prob = softmax(np.dot(self.X, W))\n",
    "        prob = np.argmax(prob, axis=1)\n",
    "        return np.sum(prob == y) / y.shape[0]\n",
    "loss_func = LossFunction(X, y)\n",
    "# Randomly initialize W, each has real value from normal distribution between -0.01 and 0.01\n",
    "W = np.random.normal(0, 0.01, (X.shape[1], 10))\n",
    "loss_func.loss(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent with line search Wolfe Condition to find the suitable learning rate\n",
    "def gradient_descent(loss_func, starting_point, num_steps = 40, precision=0.00001):\n",
    "    cur_point = starting_point\n",
    "    for i in range(num_steps):\n",
    "        initial_learning_rate = 0.001\n",
    "        grad = loss_func.gradient(cur_point)\n",
    "        while (True):\n",
    "            # Check the Wolfe condition\n",
    "            if (loss_func.loss(cur_point - initial_learning_rate * grad) <= loss_func.loss(cur_point) - 0.0001 * initial_learning_rate * np.sum(grad ** 2)):\n",
    "                break\n",
    "            initial_learning_rate = initial_learning_rate * 0.8\n",
    "        cur_point = cur_point - initial_learning_rate * grad\n",
    "        print(\"Iteration {}: loss = {}, precision = {}\".format(i, loss_func.loss(cur_point), loss_func.precision(cur_point)))\n",
    "    return cur_point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer\n",
    "def adam(loss_func, starting_point, num_steps = 40, precision=0.00001):\n",
    "    cur_point = starting_point\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "    m = np.zeros(cur_point.shape)\n",
    "    v = np.zeros(cur_point.shape)\n",
    "    for i in range(num_steps):\n",
    "        grad = loss_func.gradient(cur_point)\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "        m_hat = m / (1 - beta1 ** (i + 1))\n",
    "        v_hat = v / (1 - beta2 ** (i + 1))\n",
    "        cur_point = cur_point - 0.006 * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "        print(\"Iteration {}: loss = {}, precision = {}\".format(i, loss_func.loss(cur_point), loss_func.precision(cur_point)))\n",
    "    return cur_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss = 19.143245416851375, precision = 0.40641666666666665\n",
      "Iteration 1: loss = 18.95141746106452, precision = 0.4258166666666667\n",
      "Iteration 2: loss = 18.193093431322154, precision = 0.4371833333333333\n",
      "Iteration 3: loss = 17.982520897357954, precision = 0.4542\n",
      "Iteration 4: loss = 17.950176810570913, precision = 0.41981666666666667\n",
      "Iteration 5: loss = 14.43964654021054, precision = 0.5506833333333333\n",
      "Iteration 6: loss = 12.75789435666089, precision = 0.5841\n",
      "Iteration 7: loss = 11.383717584957864, precision = 0.63075\n",
      "Iteration 8: loss = 10.320786482906561, precision = 0.6488666666666667\n",
      "Iteration 9: loss = 10.164063305122898, precision = 0.65015\n",
      "Iteration 10: loss = 10.04669428240463, precision = 0.6635833333333333\n",
      "Iteration 11: loss = 8.403510573508607, precision = 0.7099666666666666\n",
      "Iteration 12: loss = 7.2157912038522625, precision = 0.7396166666666667\n",
      "Iteration 13: loss = 6.860263145736326, precision = 0.76525\n",
      "Iteration 14: loss = 5.1218763234519935, precision = 0.8048333333333333\n",
      "Iteration 15: loss = 4.34881242903482, precision = 0.82905\n",
      "Iteration 16: loss = 4.032966664871783, precision = 0.8391\n",
      "Iteration 17: loss = 4.004830411511899, precision = 0.8389666666666666\n",
      "Iteration 18: loss = 3.7636757498916222, precision = 0.8473166666666667\n",
      "Iteration 19: loss = 3.7448467272859403, precision = 0.8472833333333334\n",
      "Iteration 20: loss = 3.5197189996560607, precision = 0.8552333333333333\n",
      "Iteration 21: loss = 3.5169478236229605, precision = 0.8542333333333333\n",
      "Iteration 22: loss = 3.357211640685781, precision = 0.8601166666666666\n",
      "Iteration 23: loss = 3.1850867303430843, precision = 0.8669833333333333\n",
      "Iteration 24: loss = 3.1214347500755646, precision = 0.86805\n",
      "Iteration 25: loss = 3.0404317396769107, precision = 0.8717666666666667\n",
      "Iteration 26: loss = 2.9998610806594694, precision = 0.8727333333333334\n",
      "Iteration 27: loss = 2.954378139738059, precision = 0.8744\n",
      "Iteration 28: loss = 2.9246661278731114, precision = 0.8749833333333333\n",
      "Iteration 29: loss = 2.898199159120224, precision = 0.8752333333333333\n",
      "Iteration 30: loss = 2.8815178811185653, precision = 0.8753\n",
      "Iteration 31: loss = 2.879594740535192, precision = 0.87455\n",
      "Iteration 32: loss = 2.758090209597765, precision = 0.8799666666666667\n",
      "Iteration 33: loss = 2.727577315949653, precision = 0.8800166666666667\n",
      "Iteration 34: loss = 2.66369797207102, precision = 0.8829333333333333\n",
      "Iteration 35: loss = 2.6367818763732385, precision = 0.88315\n",
      "Iteration 36: loss = 2.5925414544451617, precision = 0.8845\n",
      "Iteration 37: loss = 2.5711343632077908, precision = 0.8851166666666667\n",
      "Iteration 38: loss = 2.535880589012307, precision = 0.88605\n",
      "Iteration 39: loss = 2.519392349025443, precision = 0.8860833333333333\n",
      "Iteration 40: loss = 2.489803533124701, precision = 0.8872833333333333\n",
      "Iteration 41: loss = 2.480579267589875, precision = 0.8869166666666667\n",
      "Iteration 42: loss = 2.4554248506373026, precision = 0.8879333333333334\n",
      "Iteration 43: loss = 2.416489228920763, precision = 0.8889\n",
      "Iteration 44: loss = 2.39910863254494, precision = 0.8895833333333333\n",
      "Iteration 45: loss = 2.3676948508915148, precision = 0.8902833333333333\n",
      "Iteration 46: loss = 2.354444226539186, precision = 0.8906666666666667\n",
      "Iteration 47: loss = 2.3270781195653387, precision = 0.8912\n",
      "Iteration 48: loss = 2.318384669420602, precision = 0.8911333333333333\n",
      "Iteration 49: loss = 2.293614635502815, precision = 0.8914666666666666\n",
      "Iteration 50: loss = 2.292200920213631, precision = 0.8913333333333333\n",
      "Iteration 51: loss = 2.269781895243326, precision = 0.89155\n",
      "Iteration 52: loss = 2.241808410529669, precision = 0.89265\n",
      "Iteration 53: loss = 2.221321471560427, precision = 0.8931333333333333\n",
      "Iteration 54: loss = 2.2035747575534774, precision = 0.8933\n",
      "Iteration 55: loss = 2.184112606106351, precision = 0.8937833333333334\n",
      "Iteration 56: loss = 2.1712071654327887, precision = 0.8940166666666667\n",
      "Iteration 57: loss = 2.1530223646869544, precision = 0.89455\n",
      "Iteration 58: loss = 2.142839602411166, precision = 0.89465\n",
      "Iteration 59: loss = 2.12687176871173, precision = 0.89505\n",
      "Iteration 60: loss = 2.1186048704147553, precision = 0.8951\n",
      "Iteration 61: loss = 2.106008683988818, precision = 0.89525\n",
      "Iteration 62: loss = 2.09930131664763, precision = 0.8949833333333334\n",
      "Iteration 63: loss = 2.0929258720233594, precision = 0.8951166666666667\n",
      "Iteration 64: loss = 2.0884285960618674, precision = 0.8947166666666667\n",
      "Iteration 65: loss = 2.054401757604723, precision = 0.89605\n",
      "Iteration 66: loss = 2.0516169301392737, precision = 0.8958833333333334\n",
      "Iteration 67: loss = 2.0509600924560383, precision = 0.89565\n",
      "Iteration 68: loss = 2.0175688185383005, precision = 0.8972\n",
      "Iteration 69: loss = 2.011543018109428, precision = 0.8968166666666667\n",
      "Iteration 70: loss = 1.9891992794568825, precision = 0.8977666666666667\n",
      "Iteration 71: loss = 1.9815824890171594, precision = 0.8973\n",
      "Iteration 72: loss = 1.9643039727320784, precision = 0.89835\n",
      "Iteration 73: loss = 1.9562733097785523, precision = 0.8977166666666667\n",
      "Iteration 74: loss = 1.941583770870362, precision = 0.89855\n",
      "Iteration 75: loss = 1.9340237297348695, precision = 0.8981\n",
      "Iteration 76: loss = 1.920608151811467, precision = 0.8988166666666667\n",
      "Iteration 77: loss = 1.914563467462214, precision = 0.8983666666666666\n",
      "Iteration 78: loss = 1.9017917668749236, precision = 0.8987833333333334\n",
      "Iteration 79: loss = 1.899221188415429, precision = 0.8986333333333333\n",
      "Iteration 80: loss = 1.8865118381862542, precision = 0.8991\n",
      "Iteration 81: loss = 1.8707832612543274, precision = 0.8994333333333333\n",
      "Iteration 82: loss = 1.8618020275659781, precision = 0.8995833333333333\n",
      "Iteration 83: loss = 1.8596104016210355, precision = 0.8994333333333333\n",
      "Iteration 84: loss = 1.8504797401912527, precision = 0.8993833333333333\n",
      "Iteration 85: loss = 1.837692781094963, precision = 0.8997166666666667\n",
      "Iteration 86: loss = 1.8305225262140405, precision = 0.8996166666666666\n",
      "Iteration 87: loss = 1.8183767191867402, precision = 0.8998833333333334\n",
      "Iteration 88: loss = 1.8125380032320835, precision = 0.9000833333333333\n",
      "Iteration 89: loss = 1.8014820130088736, precision = 0.9001166666666667\n",
      "Iteration 90: loss = 1.7967717986119378, precision = 0.9002\n",
      "Iteration 91: loss = 1.7875335183316785, precision = 0.90015\n",
      "Iteration 92: loss = 1.7840370114759474, precision = 0.9000166666666667\n",
      "Iteration 93: loss = 1.7782643916049776, precision = 0.90025\n",
      "Iteration 94: loss = 1.776688820465649, precision = 0.8997166666666667\n",
      "Iteration 95: loss = 1.753492884080207, precision = 0.9006666666666666\n",
      "Iteration 96: loss = 1.7513455201412274, precision = 0.9006166666666666\n",
      "Iteration 97: loss = 1.7471549407993714, precision = 0.9006666666666666\n",
      "Iteration 98: loss = 1.7295211150411902, precision = 0.9011166666666667\n",
      "Iteration 99: loss = 1.721870703839003, precision = 0.9013333333333333\n"
     ]
    }
   ],
   "source": [
    "W = gradient_descent(loss_func, W, num_steps=100, precision=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss = 22.158634900785824, precision = 0.33516666666666667\n",
      "Iteration 1: loss = 28.08851054679652, precision = 0.12391666666666666\n",
      "Iteration 2: loss = 20.28845768052143, precision = 0.31621666666666665\n",
      "Iteration 3: loss = 20.643962539152135, precision = 0.32493333333333335\n",
      "Iteration 4: loss = 19.258954756194193, precision = 0.3771833333333333\n",
      "Iteration 5: loss = 16.196410623763914, precision = 0.47551666666666664\n",
      "Iteration 6: loss = 13.510960152888686, precision = 0.5781\n",
      "Iteration 7: loss = 12.370933541004494, precision = 0.6069\n",
      "Iteration 8: loss = 10.549921999468873, precision = 0.6548166666666667\n",
      "Iteration 9: loss = 11.1391260787693, precision = 0.63635\n",
      "Iteration 10: loss = 11.69552216478553, precision = 0.6157\n",
      "Iteration 11: loss = 9.354955699178925, precision = 0.6752833333333333\n",
      "Iteration 12: loss = 6.679518881544701, precision = 0.7530666666666667\n",
      "Iteration 13: loss = 7.602369121950875, precision = 0.7243166666666667\n",
      "Iteration 14: loss = 8.627734862000429, precision = 0.6915666666666667\n",
      "Iteration 15: loss = 8.20812152033989, precision = 0.70455\n",
      "Iteration 16: loss = 7.2749856935663, precision = 0.7381333333333333\n",
      "Iteration 17: loss = 6.365016401797278, precision = 0.7692666666666667\n",
      "Iteration 18: loss = 5.725578775819336, precision = 0.79435\n",
      "Iteration 19: loss = 5.212355143724629, precision = 0.8156\n",
      "Iteration 20: loss = 4.792278086779676, precision = 0.8316666666666667\n",
      "Iteration 21: loss = 4.615463293538721, precision = 0.8395\n",
      "Iteration 22: loss = 4.645625311709595, precision = 0.8394\n",
      "Iteration 23: loss = 4.714484373066572, precision = 0.8372\n",
      "Iteration 24: loss = 4.6551561364671254, precision = 0.8385166666666667\n",
      "Iteration 25: loss = 4.38977167565403, precision = 0.8468833333333333\n",
      "Iteration 26: loss = 3.9977009751652215, precision = 0.8595833333333334\n",
      "Iteration 27: loss = 3.633592277716613, precision = 0.8711833333333333\n",
      "Iteration 28: loss = 3.409936130959043, precision = 0.88\n",
      "Iteration 29: loss = 3.3450030853732984, precision = 0.8822833333333333\n",
      "Iteration 30: loss = 3.3653005907814104, precision = 0.8819166666666667\n",
      "Iteration 31: loss = 3.3895600389606786, precision = 0.8800166666666667\n",
      "Iteration 32: loss = 3.3662433292489595, precision = 0.8812\n",
      "Iteration 33: loss = 3.2840788753596732, precision = 0.8839666666666667\n",
      "Iteration 34: loss = 3.1560515127604996, precision = 0.8880666666666667\n",
      "Iteration 35: loss = 3.0216046462470065, precision = 0.8925666666666666\n",
      "Iteration 36: loss = 2.909735896977125, precision = 0.8963666666666666\n",
      "Iteration 37: loss = 2.8218426733005626, precision = 0.8989666666666667\n",
      "Iteration 38: loss = 2.762916673128558, precision = 0.8998166666666667\n",
      "Iteration 39: loss = 2.759143751963154, precision = 0.8999\n",
      "Iteration 40: loss = 2.766320178703066, precision = 0.89855\n",
      "Iteration 41: loss = 2.777396851335018, precision = 0.8975333333333333\n",
      "Iteration 42: loss = 2.772344349905281, precision = 0.89845\n",
      "Iteration 43: loss = 2.736287673706224, precision = 0.8994166666666666\n",
      "Iteration 44: loss = 2.6726027434711948, precision = 0.9015\n",
      "Iteration 45: loss = 2.6029015335266448, precision = 0.90405\n",
      "Iteration 46: loss = 2.543512328804734, precision = 0.9058666666666667\n",
      "Iteration 47: loss = 2.501509184889393, precision = 0.9074333333333333\n",
      "Iteration 48: loss = 2.464779051443887, precision = 0.9085\n",
      "Iteration 49: loss = 2.433887696191589, precision = 0.90965\n",
      "Iteration 50: loss = 2.4081341612198823, precision = 0.9102666666666667\n",
      "Iteration 51: loss = 2.387576258155328, precision = 0.9105666666666666\n",
      "Iteration 52: loss = 2.3708050269475383, precision = 0.9108\n",
      "Iteration 53: loss = 2.3505801181998858, precision = 0.9114666666666666\n",
      "Iteration 54: loss = 2.3231172641409694, precision = 0.9129333333333334\n",
      "Iteration 55: loss = 2.2908181451109204, precision = 0.91385\n",
      "Iteration 56: loss = 2.261610024401636, precision = 0.91465\n",
      "Iteration 57: loss = 2.237698784713725, precision = 0.9148\n",
      "Iteration 58: loss = 2.225116672180932, precision = 0.9148333333333334\n",
      "Iteration 59: loss = 2.2193473738684424, precision = 0.9145\n",
      "Iteration 60: loss = 2.214461263149087, precision = 0.9145333333333333\n",
      "Iteration 61: loss = 2.198738992226604, precision = 0.9146166666666666\n",
      "Iteration 62: loss = 2.1670724336697322, precision = 0.91535\n",
      "Iteration 63: loss = 2.1254085525968294, precision = 0.9167666666666666\n",
      "Iteration 64: loss = 2.0866649679072378, precision = 0.9179166666666667\n",
      "Iteration 65: loss = 2.058658380705996, precision = 0.9188166666666666\n",
      "Iteration 66: loss = 2.0418727986192264, precision = 0.9189833333333334\n",
      "Iteration 67: loss = 2.031135672122916, precision = 0.9190333333333334\n",
      "Iteration 68: loss = 2.020698614493272, precision = 0.9192333333333333\n",
      "Iteration 69: loss = 2.0043893112744646, precision = 0.9197\n",
      "Iteration 70: loss = 1.9838659984894964, precision = 0.9197333333333333\n",
      "Iteration 71: loss = 1.966621319848238, precision = 0.9197166666666666\n",
      "Iteration 72: loss = 1.957411981246119, precision = 0.9201\n",
      "Iteration 73: loss = 1.9539884773682885, precision = 0.9201166666666667\n",
      "Iteration 74: loss = 1.9507102675477404, precision = 0.9203\n",
      "Iteration 75: loss = 1.9437355261375344, precision = 0.9203166666666667\n",
      "Iteration 76: loss = 1.9315532969472524, precision = 0.9203333333333333\n",
      "Iteration 77: loss = 1.915662128677385, precision = 0.92045\n",
      "Iteration 78: loss = 1.8981338973120694, precision = 0.9209833333333334\n",
      "Iteration 79: loss = 1.879762208232754, precision = 0.9208666666666666\n",
      "Iteration 80: loss = 1.8629761854664457, precision = 0.9212833333333333\n",
      "Iteration 81: loss = 1.849542560998153, precision = 0.9213166666666667\n",
      "Iteration 82: loss = 1.8385345157733912, precision = 0.921\n",
      "Iteration 83: loss = 1.8272698603263398, precision = 0.9213\n",
      "Iteration 84: loss = 1.8141878701754408, precision = 0.9213833333333333\n",
      "Iteration 85: loss = 1.8000069307701718, precision = 0.9218833333333334\n",
      "Iteration 86: loss = 1.7866754756671455, precision = 0.9221166666666667\n",
      "Iteration 87: loss = 1.7748271724755098, precision = 0.92225\n",
      "Iteration 88: loss = 1.7634833349306078, precision = 0.9221666666666667\n",
      "Iteration 89: loss = 1.751004259769039, precision = 0.9226\n",
      "Iteration 90: loss = 1.7364282489596254, precision = 0.9223333333333333\n",
      "Iteration 91: loss = 1.7214928746339662, precision = 0.92265\n",
      "Iteration 92: loss = 1.7076411580677844, precision = 0.9227333333333333\n",
      "Iteration 93: loss = 1.6949000641459837, precision = 0.9229333333333334\n",
      "Iteration 94: loss = 1.683256161155897, precision = 0.923\n",
      "Iteration 95: loss = 1.6722361075458259, precision = 0.923\n",
      "Iteration 96: loss = 1.6608366175507248, precision = 0.9231166666666667\n",
      "Iteration 97: loss = 1.6483321081660458, precision = 0.9235333333333333\n",
      "Iteration 98: loss = 1.6354633166333516, precision = 0.92375\n",
      "Iteration 99: loss = 1.6236330020324035, precision = 0.9242666666666667\n",
      "Iteration 100: loss = 1.612573469193791, precision = 0.9242333333333334\n",
      "Iteration 101: loss = 1.601050579454961, precision = 0.92425\n",
      "Iteration 102: loss = 1.5888866341164225, precision = 0.9245666666666666\n",
      "Iteration 103: loss = 1.5769141127418589, precision = 0.9247166666666666\n",
      "Iteration 104: loss = 1.5655963861593085, precision = 0.9251666666666667\n",
      "Iteration 105: loss = 1.5545344773831509, precision = 0.9254333333333333\n",
      "Iteration 106: loss = 1.5436471972222447, precision = 0.92545\n",
      "Iteration 107: loss = 1.5327037897810212, precision = 0.92535\n",
      "Iteration 108: loss = 1.5217555996698255, precision = 0.92535\n",
      "Iteration 109: loss = 1.5113990034895974, precision = 0.92535\n",
      "Iteration 110: loss = 1.5016181016579138, precision = 0.9255\n",
      "Iteration 111: loss = 1.4918690107438923, precision = 0.9257\n",
      "Iteration 112: loss = 1.4817619302864393, precision = 0.9257333333333333\n",
      "Iteration 113: loss = 1.471609976471423, precision = 0.9258166666666666\n",
      "Iteration 114: loss = 1.4617432988341563, precision = 0.9259666666666667\n",
      "Iteration 115: loss = 1.4519813616735489, precision = 0.9259833333333334\n",
      "Iteration 116: loss = 1.4421654280757326, precision = 0.9258833333333333\n",
      "Iteration 117: loss = 1.4322143960465248, precision = 0.9259666666666667\n",
      "Iteration 118: loss = 1.4221709001315246, precision = 0.9261166666666667\n",
      "Iteration 119: loss = 1.4124431198756038, precision = 0.9263333333333333\n",
      "Iteration 120: loss = 1.4028344475883314, precision = 0.9262333333333334\n",
      "Iteration 121: loss = 1.3930815343322585, precision = 0.9261166666666667\n",
      "Iteration 122: loss = 1.3831971710349704, precision = 0.9260833333333334\n",
      "Iteration 123: loss = 1.3733545626702544, precision = 0.9261833333333334\n",
      "Iteration 124: loss = 1.3634925966813887, precision = 0.9262666666666667\n",
      "Iteration 125: loss = 1.3536376930990115, precision = 0.9262\n",
      "Iteration 126: loss = 1.3439497708514607, precision = 0.9262833333333333\n",
      "Iteration 127: loss = 1.3346792077594576, precision = 0.9262666666666667\n",
      "Iteration 128: loss = 1.3256792800396904, precision = 0.9262666666666667\n",
      "Iteration 129: loss = 1.3165806977846546, precision = 0.9262\n",
      "Iteration 130: loss = 1.3073111955146726, precision = 0.9260833333333334\n",
      "Iteration 131: loss = 1.2980603874376124, precision = 0.9261166666666667\n",
      "Iteration 132: loss = 1.28894861923384, precision = 0.9263333333333333\n",
      "Iteration 133: loss = 1.279916562529361, precision = 0.92615\n",
      "Iteration 134: loss = 1.2708404393992965, precision = 0.9263666666666667\n",
      "Iteration 135: loss = 1.2619350808218062, precision = 0.9264666666666667\n",
      "Iteration 136: loss = 1.2532324783951279, precision = 0.9264333333333333\n",
      "Iteration 137: loss = 1.2444963050783782, precision = 0.9263833333333333\n",
      "Iteration 138: loss = 1.2357039311409321, precision = 0.9262166666666667\n",
      "Iteration 139: loss = 1.2268017608233501, precision = 0.9265833333333333\n",
      "Iteration 140: loss = 1.2180634387650477, precision = 0.9266\n",
      "Iteration 141: loss = 1.2095711527874606, precision = 0.9264666666666667\n",
      "Iteration 142: loss = 1.2012362457478887, precision = 0.9266333333333333\n",
      "Iteration 143: loss = 1.1928814936771885, precision = 0.9266833333333333\n",
      "Iteration 144: loss = 1.1844149342231391, precision = 0.92665\n",
      "Iteration 145: loss = 1.1759009044128315, precision = 0.9264666666666667\n",
      "Iteration 146: loss = 1.167402341581531, precision = 0.9263666666666667\n",
      "Iteration 147: loss = 1.1589309828859773, precision = 0.9261833333333334\n",
      "Iteration 148: loss = 1.1505696260076714, precision = 0.9261333333333334\n",
      "Iteration 149: loss = 1.1423333105050282, precision = 0.9263166666666667\n",
      "Iteration 150: loss = 1.1340716764236922, precision = 0.9265\n",
      "Iteration 151: loss = 1.12580886271329, precision = 0.9266\n",
      "Iteration 152: loss = 1.117635305278696, precision = 0.9264833333333333\n",
      "Iteration 153: loss = 1.1096014966019283, precision = 0.9264\n",
      "Iteration 154: loss = 1.1017486765850202, precision = 0.9266\n",
      "Iteration 155: loss = 1.0940166016474966, precision = 0.9267166666666666\n",
      "Iteration 156: loss = 1.0862211243014273, precision = 0.9266333333333333\n",
      "Iteration 157: loss = 1.0784014155753872, precision = 0.9267\n",
      "Iteration 158: loss = 1.0706700793206754, precision = 0.9266\n",
      "Iteration 159: loss = 1.0629938849139489, precision = 0.9266833333333333\n",
      "Iteration 160: loss = 1.0554192514737182, precision = 0.9267166666666666\n",
      "Iteration 161: loss = 1.0478973250090735, precision = 0.9266\n",
      "Iteration 162: loss = 1.0403162923655787, precision = 0.9266166666666666\n",
      "Iteration 163: loss = 1.0327784161149236, precision = 0.9266166666666666\n",
      "Iteration 164: loss = 1.0254063151037833, precision = 0.9264833333333333\n",
      "Iteration 165: loss = 1.018200025857069, precision = 0.9267666666666666\n",
      "Iteration 166: loss = 1.0110577213724328, precision = 0.9267\n",
      "Iteration 167: loss = 1.0038716139766866, precision = 0.9266833333333333\n",
      "Iteration 168: loss = 0.9966904219458498, precision = 0.92665\n",
      "Iteration 169: loss = 0.9895871419011851, precision = 0.9266333333333333\n",
      "Iteration 170: loss = 0.9825084020129188, precision = 0.9266833333333333\n",
      "Iteration 171: loss = 0.9754494092676164, precision = 0.9264833333333333\n",
      "Iteration 172: loss = 0.9684037275292606, precision = 0.9265333333333333\n",
      "Iteration 173: loss = 0.9614037623193531, precision = 0.9266833333333333\n",
      "Iteration 174: loss = 0.9544657783213756, precision = 0.9265333333333333\n",
      "Iteration 175: loss = 0.9475970188919439, precision = 0.9264333333333333\n",
      "Iteration 176: loss = 0.9407374525690957, precision = 0.9265166666666667\n",
      "Iteration 177: loss = 0.9338928323252964, precision = 0.9263166666666667\n",
      "Iteration 178: loss = 0.9271261783412207, precision = 0.9264\n",
      "Iteration 179: loss = 0.9204483380272268, precision = 0.9264666666666667\n",
      "Iteration 180: loss = 0.9138122302308475, precision = 0.9264666666666667\n",
      "Iteration 181: loss = 0.9072187945272554, precision = 0.9262833333333333\n",
      "Iteration 182: loss = 0.9006758185821298, precision = 0.9261333333333334\n",
      "Iteration 183: loss = 0.894186664357823, precision = 0.9262833333333333\n",
      "Iteration 184: loss = 0.8877323378451899, precision = 0.9263333333333333\n",
      "Iteration 185: loss = 0.8812732715975133, precision = 0.9263\n",
      "Iteration 186: loss = 0.8748429019799143, precision = 0.9261666666666667\n",
      "Iteration 187: loss = 0.8684776155709089, precision = 0.9259333333333334\n",
      "Iteration 188: loss = 0.8621481573874288, precision = 0.9258666666666666\n",
      "Iteration 189: loss = 0.8558711398735547, precision = 0.9258166666666666\n",
      "Iteration 190: loss = 0.8496878066021022, precision = 0.9257\n",
      "Iteration 191: loss = 0.8435603642865794, precision = 0.9259\n",
      "Iteration 192: loss = 0.8374846090104194, precision = 0.9261833333333334\n",
      "Iteration 193: loss = 0.8314610664348759, precision = 0.92625\n",
      "Iteration 194: loss = 0.8254973858687811, precision = 0.9263166666666667\n",
      "Iteration 195: loss = 0.8196035783769007, precision = 0.92635\n",
      "Iteration 196: loss = 0.8137205504591594, precision = 0.9265166666666667\n",
      "Iteration 197: loss = 0.8078898814416214, precision = 0.9264333333333333\n",
      "Iteration 198: loss = 0.8021332533380586, precision = 0.9265166666666667\n",
      "Iteration 199: loss = 0.796394397286031, precision = 0.9265\n"
     ]
    }
   ],
   "source": [
    "W = adam(loss_func, W, num_steps=200, precision=0.00001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
